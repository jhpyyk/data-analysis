{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Nested cross-validation exercise\n",
    "## Nested cross-validation for feature selection with nearest neighbors <br>\n",
    "- Use Python 3 to program both a hyper-parameter selection method based on leave-one-out cross-validation and a nested leave-one-out cross-validation for estimating the prediction performance of models inferred with this automatic selection approach. Use base learning algorithm provided in the exercise, namely the \"use_ith_feature\" method, so that the value of the hyper-parameter i is automatically selected from the range from 1 to 100 of alternative values. The provided base learning algorithm \"use_ith_feature\" is 1-nearest neighbor that uses only the ith feature of the data given to it. The LOOCV based hyper-parameter selection procedure is supposed to select the best feature, e.g. the value of i, based on C-index evaluated with predictions obtained with leave-one-out cross-validation. A ready-made implementation of C-index is also provided in the exercise. In nested leave-one-out cross-validation, a C_index value is further evaluated on the predictions obtained from an outer leave-one-out cross-validation. During each round of this outer LOOCV, the whole feature selection process based on inner LOOCV is separately done and the selected feature is used for prediction for the test data point held out during that round of the outer LOOCV. Accordingly, the actual learning algorithm, whose prediction performance will be evaluated with nested CV, is the one that automatically selects the single best feature with leave-one-out cross-validation based model selection (see the lectures and the pseudo codes presented on them for more info on this interpretation).\n",
    "- Note that since the hold-out set in LOOCV has only a single datum but C-index requires at least two data points. The solution in this exercise is to \"pool\" the predictions of all LOOCV rounds of a single LOOCV computation into an array of length of the data used in that LOOCV computation and then compute C-index on that array and the corresponding true outputs. This pooling approach, however, does have its weaknesses, since C-index computed from pooled LOOCV outputs may sometimes be a heavily biased estimator of the true C-index. This has been considered in detail in our previous research (and other group's too as seen in the references) that is available here:\n",
    "http://dx.doi.org/10.1177/0962280218795190\n",
    "where AUC, a special case of C-index, is considered. The study goes quite deep into the problem of AUC estimation with CV, and you can read it if you are interested about the research carried out in our laboratory, while EMLM course does not go that far and this year's exercise unfortunately still has this non-optimal pooling approach in use.\n",
    "- Compare the C-index produced by nested leave-one-out CV with the result of ordinary leave-one-out CV with the best value of i e.g. the feature providing the highest LOOCV C-index, and show the C-index difference between the two methods.\n",
    "- Use the provided implementation of the \"use_ith_feature\" learning algorithm and C-index functions in your exercise.\n",
    "\n",
    "As a summary, for completing this exercise implement the following steps: \n",
    "_______________________________________________________________\n",
    "#### 1. Use leave-one-out cross-validation for determining the optimal i-parameter for the data (X_alternative.csv, y_alternative.csv) from the set of possible values of i e.g. {1,...,100}. When you have found the optimal i, save the corresponding C-index (call it loo_c_index) for this parameter.\n",
    "#### 2. Similarly, use nested leave-one-out cross-validation (leave-one-out both in outer and inner folds) for estimating the C-index (call it nloo_c_index) of the method that selects the best feature with leave-one-out approach. \n",
    "#### 3. Return both this notebook and as a PDF-file made from it in the exercise submit page. \n",
    "_______________________________________________________________\n",
    "\n",
    "Remember to use the provided learning algorithm use_ith_feature and C-index functions in your implementation! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell import all libraries you need. For example: \n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "C-index function: \n",
    "- INPUTS: \n",
    "'y' an array of the true output values\n",
    "'yp' an array of predicted output values\n",
    "- OUTPUT: \n",
    "The c-index value\n",
    "\"\"\"\n",
    "def cindex(y, yp):\n",
    "    n = 0\n",
    "    h_num = 0 \n",
    "    for i in range(0, len(y)):\n",
    "        t = y[i]\n",
    "        p = yp[i]\n",
    "        for j in range(i+1, len(y)):\n",
    "            nt = y[j]\n",
    "            np = yp[j]\n",
    "            if (t != nt): \n",
    "                n = n + 1\n",
    "                if (p < np and t < nt) or (p > np and t > nt): \n",
    "                    h_num += 1\n",
    "                elif (p == np):\n",
    "                    h_num += 0.5\n",
    "    return h_num/n\n",
    "\n",
    "\"\"\"\n",
    "Self-contained 1-nearest neighbor using only a single feature\n",
    "- INPUTS: \n",
    "'X_train' a numpy matrix of the X-features of the train data points\n",
    "'y_train' a numpy matrix of the output values of the train data points\n",
    "'X_test' a numpy matrix of the X-features of the test data points\n",
    "'i' the index of the feature to be used with 1-nearest neighbor\n",
    "- OUTPUT: \n",
    "'y_predictions' a list of the output value predictions\n",
    "\"\"\"\n",
    "def use_ith_feature(X_train, y_train, X_test, i):\n",
    "    y_predictions = []\n",
    "    for test_ind in range(0, X_test.shape[0]):\n",
    "        diff = X_test[test_ind, i] - X_train[:, i]\n",
    "        distances = np.sqrt(diff * diff)\n",
    "        sort_inds = np.array(np.argsort(distances), dtype=int)\n",
    "        y_predictions.append(y_train[sort_inds[0]])\n",
    "    return y_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your implementation here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Load data from csv-files into numpy arrays\n",
    "data_X = np.genfromtxt('X_alternative.csv', delimiter=',')\n",
    "data_y = np.genfromtxt('y_alternative.csv', delimiter=',')\n",
    "\n",
    "# Check for NaN values\n",
    "print(np.any(np.isnan(data_X)))\n",
    "print(np.any(np.isnan(data_y)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong answer with this. New implementation in the next cell.\n",
    "\n",
    "def get_optimal_i(data_X, data_y):\n",
    "    # Scikit-learn leave-one-out\n",
    "    loo = LeaveOneOut()\n",
    "    splits = loo.split(data_X)\n",
    "\n",
    "    pred_pool = []\n",
    "    true_y_pool = []\n",
    "\n",
    "    c_indices = []\n",
    "\n",
    "    # Loop over all of the splits and calculate the C-index for each split\n",
    "    for (train_index, test_index) in splits:\n",
    "        # Predictions for this split\n",
    "        pred_this_split = []\n",
    "        for i in range(100):\n",
    "            # Predictions using ith feature\n",
    "            pred_i = use_ith_feature(data_X[train_index], data_y[train_index], data_X[test_index], i)\n",
    "            # Reduce extra dimension and append\n",
    "            pred_this_split.append(pred_i[0])\n",
    "\n",
    "        # Add predictions to the pool\n",
    "        pred_pool.append(pred_this_split)\n",
    "        true_y_pool.append(data_y[test_index][0])\n",
    "    \n",
    "    # Calculate C-index for each prediction array\n",
    "    for i in range(len(true_y_pool)):\n",
    "        c_ind = cindex(true_y_pool, pred_pool[:][i])\n",
    "        c_indices.append(c_ind)\n",
    "\n",
    "    # Get index of highest C-index value\n",
    "    opt_i = np.argmax(c_indices)\n",
    "    best_c_index = c_indices[opt_i]\n",
    "    return opt_i, best_c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_i2(data_X, data_y):\n",
    "\n",
    "\n",
    "    c_index_for_each_i = []\n",
    "\n",
    "    for i in range(100):\n",
    "\n",
    "        # Scikit-learn leave-one-out\n",
    "        loo = LeaveOneOut()\n",
    "        splits = loo.split(data_X)\n",
    "\n",
    "        # Predictions for this i\n",
    "        pred_this_i = []\n",
    "        # True labels for this i\n",
    "        y_true_this_i = []\n",
    "\n",
    "        # Loop over all of the splits\n",
    "        for (train_index, test_index) in splits:\n",
    "            # Prediction using ith feature\n",
    "            pred_i = use_ith_feature(data_X[train_index], data_y[train_index], data_X[test_index], i)\n",
    "            # Reduce extra dimension and append prediction and true label\n",
    "            pred_this_i.append(pred_i[0])\n",
    "            y_true_this_i.append(data_y[test_index][0])\n",
    "\n",
    "        # Calculate C-index for this i\n",
    "        c_ind_this_i = cindex(y_true_this_i, pred_this_i)\n",
    "        c_index_for_each_i.append(c_ind_this_i)\n",
    "\n",
    "\n",
    "    optimal_i = np.argmax(c_index_for_each_i)\n",
    "    c_index_for_optimal_i = c_index_for_each_i[optimal_i]\n",
    "    return optimal_i, c_index_for_optimal_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal i:  76\n",
      "C-index for optimal i:  0.6620689655172414\n"
     ]
    }
   ],
   "source": [
    "optimal_i, loo_c_index = get_optimal_i2(data_X, data_y)\n",
    "print(\"Optimal i: \", optimal_i)\n",
    "print(\"C-index for optimal i: \", loo_c_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5149425287356322"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-learn leave-one-out\n",
    "outer_loo = LeaveOneOut()\n",
    "outer_splits = outer_loo.split(data_X)\n",
    "\n",
    "outer_pred_pool = []\n",
    "outer_true_y_pool = []\n",
    "\n",
    "for (train_index, test_index) in outer_splits:\n",
    "    opt_i = get_optimal_i2(data_X[train_index], data_y[train_index])[0]\n",
    "    # Prediction with the optimal i\n",
    "    pred = use_ith_feature(data_X[train_index], data_y[train_index], data_X[test_index], opt_i)\n",
    "\n",
    "    # Add prediction and true value to the pools\n",
    "    outer_pred_pool.append(pred[0])\n",
    "    outer_true_y_pool.append(data_y[test_index])\n",
    "\n",
    "# Calculate C-index from the pools\n",
    "nloo_c_index = cindex(outer_true_y_pool, outer_pred_pool)\n",
    "nloo_c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index for the first method is: 0.6620689655172414.\n",
      "C-index for the second method is: 0.5149425287356322.\n"
     ]
    }
   ],
   "source": [
    "print(\"C-index for the first method is: {}.\".format(loo_c_index))\n",
    "print(\"C-index for the second method is: {}.\".format(nloo_c_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C-index for the second method implies that the learning algorithm is as good as a random guess."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
